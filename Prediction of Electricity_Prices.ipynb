{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense, Dropout, Conv1D, MaxPool1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras import backend as be\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import absl.logging\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam, SGD, RMSprop, Adamax, Nadam\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.initializers import glorot_normal\n",
    "from keras.constraints import maxnorm\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as be\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import TimeDistributed, Reshape\n",
    "from keras.layers.merge import concatenate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as be\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration data used in this excercise is already preprocessed and split into\n",
    "# training, validation, and test sets\n",
    "\n",
    "# Use wget to download the data stored in csv format.\n",
    "import itertools\n",
    "\n",
    "# Define what files to download; download all of the preprocessed data\n",
    "# Note that the data are already split into Train, Validation, and Test sets.\n",
    "# The predictor data are denoted with 'X', the target by 'y'\n",
    "data_download = {}\n",
    "data_download[\"window_size\"] = [5, 15]\n",
    "data_download[\"data_type\"] = [\"train\", \"valid\", \"test\"]\n",
    "data_download[\"predictor_or_target\"] = [\"X\", \"y\"]\n",
    "\n",
    "# Prepare the combinations of the window sizes and the data types\n",
    "keys, values = zip(*data_download.items())\n",
    "data_download_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "display('The kind of data to be downloaded:', data_download_combinations)\n",
    "\n",
    "print(\"Downloading started...\")\n",
    "for data_download_param in data_download_combinations:\n",
    "  file_to_download = \"https://frankfurt-school-dataset.s3.eu-central-1.amazonaws.com/Sept2021/window_size_{0}_time_encoding_True/{1}_{2}_window_size_{0}_time_encoding_True.csv\"\\\n",
    "                     .format(data_download_param[\"window_size\"], data_download_param[\"predictor_or_target\"], data_download_param[\"data_type\"])\n",
    "\n",
    "  # the actual downloading\n",
    "  !wget \"$file_to_download\"\n",
    "\n",
    "print(\"Downloading has finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the data with time_window\n",
    "def load_data(num):\n",
    "\n",
    "    X_train = pd.read_csv(\"X_train_window_size_{}_time_encoding_True.csv\".format(num))\n",
    "    y_train = pd.read_csv(\"y_train_window_size_{}_time_encoding_True.csv\".format(num))\n",
    "\n",
    "    X_valid = pd.read_csv(\"X_valid_window_size_{}_time_encoding_True.csv\".format(num))\n",
    "    y_valid = pd.read_csv(\"y_valid_window_size_{}_time_encoding_True.csv\".format(num))\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 5\n",
    "X_train, y_train, X_valid, y_valid = load_data(time_window)\n",
    "\n",
    "# lets have a look at target variable patter across day (assuming weekdays =7)\n",
    "df_x = X_train\n",
    "df_x['target'] = y_train['y']\n",
    "df_x['weekday'] = np.arcsin(df_x.dlvry_weekday_sin)\n",
    "df_x['temp'] =df_x.weekday *7 /(2*np.pi)\n",
    "\n",
    "# Force the histogram to bin the days into 7 bins, though this will not give exact return to 7 days,\n",
    "# or maybe it does. lets see by giving bins = 10\n",
    "\n",
    "a, b = np.histogram(df_x.temp,bins=10)\n",
    "a = np.digitize(df_x.temp, bins=b)\n",
    "df_x['weekday'] = pd.DataFrame(a)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "sns.boxplot(df_x.weekday, df_x[\"target\"], ax=ax)\n",
    "plt.title(\"Values for weekdays\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.yscale(\"symlog\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Seen even though we tried to put the data in 10 bins we are getting 7 because other bins are empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lets try to visualize the data for hour, assuming the deliveryhour window = (1,24)\n",
    "# Again as above we can force the hsitorgram to create more than 24 bins and see what we get.\n",
    "\n",
    "df_x['hr'] = np.arcsin(df_x.dlvry_hour_sin)\n",
    "df_x['temp'] =df_x.hr *24 /(2*np.pi)\n",
    "\n",
    "a, b = np.histogram(df_x.temp,bins=30)\n",
    "a = np.digitize(df_x.temp, bins=b)\n",
    "df_x['hr'] = pd.DataFrame(a)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "sns.boxplot(df_x.hr, df_x[\"target\"], ax=ax)\n",
    "plt.title(\"Values for hours\")\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.yscale(\"symlog\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delivery Hour has lesser number of bins than 24, maybe because of the difference between delivery time window and contract trading hours not sharing same time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable (contract difference) plot.\n",
    "plot = y_train[\"y\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though its evident from graph that the series is stationary. Lets do the test:\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "result = adfuller(y_train[\"y\"].values)\n",
    "\n",
    "print('p-value: %f' % result[1])\n",
    "\n",
    "print(\"Should be under 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# lets look at partial auto correlation for the series first for 5 and then 15.\n",
    "\n",
    "plot_acf(df_x[\"target\"].values, lags=30 , zero=False)  # method = \"ldadjusted\"\n",
    "plt.xlabel(\"Measurement points back in time - relatively\")\n",
    "plt.yscale(\"symlog\")\n",
    "plt.show()\n",
    "plot_pacf(df_x[\"target\"].values, lags=30\n",
    ", zero=False)\n",
    "plt.xlabel(\"Measurement points back in time - relatively\")\n",
    "plt.show()\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF and ACF for Time window 15\n",
    "X_train_15, y_train_15,_,_ = load_data(15)\n",
    "plt.close()\n",
    "plot_acf(y_train_15[\"y\"].values, lags=30 , zero=False) \n",
    "plt.xlabel(\"Measurement points back in time - relatively\")\n",
    "# plt.yscale(\"symlog\")\n",
    "plt.show()\n",
    "plot_pacf(y_train_15[\"y\"].values, lags=30\n",
    ", zero=False)\n",
    "plt.xlabel(\"Measurement points back in time - relatively\")\n",
    "plt.show()\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the some correlation than compared to time window 5 series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "\n",
    "# Partial correlation of the Candles columns\n",
    "# Except for TimeFromLastTradeDiff which will obviously have coorelation other variables have \n",
    "# correlation in window of <10 hrs but Volumediff has some jumps beyond this window.\n",
    "\n",
    "columns = ['0', '1', '2', '3', '4', '5']\n",
    "list_ = ['OpenDiff', 'HighDiff','LowDiff','CloseDiff','VolumeDiff','TimeFromlasttradeDiff']\n",
    "for col in columns:\n",
    "    plt.figure()\n",
    "    plot_pacf(X_train[col].values, lags=72, zero=False)\n",
    "    plt.title(list_[int(col)])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these graph tell us that some coorelation is there  in initial hours only but the significance is very less. As per the grpah it looks that the target has very less correlation after the initial hours of 15hrs. As the data by nature of rolling window has the target variable of ith row in the input of ith + 1 row, and since we cannot use this info in any manner. However if this was not case we then we can roll the data with window of 15 hrs and pass this to the models (either as it si or by flattening in case of statistical models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_history(history,name =\"None\"):   # As we are interested mainy in Loss we shamelessly copy the code from lecture\n",
    "                                            # notebook and use it by doing slight modification\n",
    "    import matplotlib.pyplot as plt\n",
    "    \"\"\"Summarize history for accuracy and loss.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(ncols=1,figsize=(10,6),sharey='row')\n",
    "    title = \"Loss_for_{}\".format(name)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'valid'], loc='upper left')\n",
    "    fig.savefig(title + '.png')                       # Save the graphs, if not then please comment out this line.\n",
    "    plt.show()                                          #it will be saved with default name \"None\", if no anme passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only Mean Squared error and Mean Absolute error as instructed for model evaluation.\n",
    "\n",
    "def evaluate_model(model,X_valid, y_valid):\n",
    "    from math import sqrt\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    predictions = model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, predictions)\n",
    "    mae = mean_absolute_error(y_valid, predictions)\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Linear Regression Models \n",
    "\n",
    "At first, we decided to run some classical linear regression models. They are as follows:\n",
    "LinearRegression, LassoRegression, RidgeRegression, ElasticNet, PassiveAggresiveRegressor, RANSACRegressor, StochasticGD <br>\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model <br>\n",
    "\n",
    "**Linear Regression**: <br>\n",
    "LinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "**Lasso Regression**: https://www.mygreatlearning.com/blog/understanding-of-lasso-regression/#introduction <br>\n",
    "We chose Lasso regression as a technique since it involves usage of fewer parameters and thereby can help in dealing with the multicollinearity problem. It uses shrinkage - i.e. data values are shrunk towards a central point as the mean. \n",
    "\n",
    "**Ridge Regression**: <br>\n",
    "https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Ridge_Regression.pdf <br>\n",
    "We used Ridge Regression as a technique not only to correct for multicollinearity but also to avoid large variances from the true values. This would also ensure the reduction in standard errors. \n",
    "\n",
    "**Elastic Net**: <br>\n",
    "https://en.wikipedia.org/wiki/Elastic_net_regularization <br>\n",
    "We used elastic as a regularized regression method since it involves a combination of the Lasso and Ridge methods (it linearly combines the L1 and L2 penalties).\n",
    "\n",
    "**Passive Aggressive Regression**<br>\n",
    "https://thecleverprogrammer.com/2021/07/04/passive-aggressive-regression-in-machine-learning/<br>\n",
    "We used Passive Aggressive Regression algorithm since it can work with real time data and continuously learns new data as it comes. This is particulary suitable for our problem and hence generalizable.\n",
    "\n",
    "**RANSAC Regression** <br>\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#ransac-regression<br>\n",
    "RANSAC is a non-deterministic algorithm producing only a reasonable result with a certain probability, which is dependent on the number of iterations. It is typically used for linear and non-linear regression problems and is especially popular in the field of photogrammetric computer vision.\n",
    "\n",
    "**SGD Regressor**: <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html?highlight=sgd%20r#sklearn.linear_model.SGDRegressor <br>\n",
    "We implemented SGD (Stochastic Gradient Descent) since it is well suited for regression problems with large number of training samples and supports different kinds of loss functions and penalties to fit the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    \n",
    "    models=dict()\n",
    "    models['LinearRegression'] = LinearRegression()\n",
    "    models['LassoRegression'] = Lasso(alpha = 0.15,random_state = 10) #alpha = 0.15 comes from a grid search!\n",
    "    models['RidgeRegression'] = Ridge(random_state = 10)\n",
    "    models['ElasticNet'] = ElasticNet(alpha = 1, l1_ratio = 0.14, random_state = 10) #value of alpha and l1_ratio is based on a grid search on this model!\n",
    "    models['PassiveAggresive_Reg'] = PassiveAggressiveRegressor(max_iter=1000, tol=1e-3,random_state = 10)\n",
    "    models['RANSACRegressor'] = RANSACRegressor(random_state = 10)\n",
    "    models['StochasticGD'] = SGDRegressor(loss = 'epsilon_insensitive', max_iter= 1000,\n",
    "                                          tol=1e-20, shuffle = False, learning_rate = \"adaptive\",random_state = 10)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()\n",
    "\n",
    "report = pd.DataFrame(columns = ['Time_window','model','Val_ms_error', 'Val_ma_error'])  \n",
    "time_window = [5,15]\n",
    "for i in time_window:\n",
    "    X_train, y_train, X_valid, y_valid = load_data(i)\n",
    "    print(\"This is for time_window{}\".format(i))\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train.values,y_train.values.reshape(y_train.shape[0],))   \n",
    "        mse_error, mae_error = evaluate_model(model,X_valid,y_valid)\n",
    "        report.loc[len(report)] = [i,name,mse_error,mae_error]\n",
    "        \n",
    "        print(\"MSE on valid for {}: {}\".format(name,mse_error))\n",
    "        print(\"MAE on valid for {}: {}\".format(name,mae_error))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the performance of all \n",
    "report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion from Regression models analysis:\n",
    "We can see from the results that in time window 5 and 15 the two regression models **Elastic Net** and **Lasso Regression** are performing the best among the implemented 7 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following two cells are a grid search for the value which is written in the get_models function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the value in the actual model is taken from this step!\n",
    "\n",
    "from numpy import arange\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = [ 0.0, 1.0, 10.0]\n",
    "model = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=-1)\n",
    "# fit model\n",
    "model.fit(X_train,y_train)\n",
    "# summarize chosen configuration\n",
    "print('alpha: %f' % model.alpha_)\n",
    "print('l1_ratio_: %f' % model.l1_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# define model evaluation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "model = LassoCV(alphas=arange(0, 1, 0.01), cv=cv, n_jobs=-1)\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "# summarize chosen configuration\n",
    "print('alpha: %f' % model.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN can extract the feature we have some categorical variable for which it can learn some dependencies on its own\n",
    "#like the Bankholiday, or Weekday or Hourday for which this contract is.Further by using the FC network we can use \n",
    "#these features to extrapolate the target variable.\n",
    "# lets implement the CNN model. This is a basic model. As can be seen from param grid. Some of the paramters \n",
    "# had been selected by doing the grid search which is omitted in analysis but the code for samme can be found \n",
    "# the  notebook below. As CNN can extract the feature we have some categorical variable for which it can learn\n",
    "# some dependencies on its own like the Bankholiday, or Weekday or Hourday for which this contract is.\n",
    "# deualt vlaue of n_features is kept for time_window =5, which can be overwittten by passing feature value \n",
    "# for time_window = 15\n",
    "\n",
    "def CNN_basemodel(n_features=41):\n",
    "\n",
    "    from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, MaxPool1D, Flatten, AveragePooling1D\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n",
    "    from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "    from tensorflow.keras.regularizers import l1\n",
    "    from tensorflow.keras.backend import clear_session\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    from tensorflow.keras.initializers import glorot_normal\n",
    "    from keras.constraints import maxnorm\n",
    "    from tensorflow.keras import initializers\n",
    "    import tensorflow as tf\n",
    "\n",
    "    x = Input(shape = (1,n_features))    # data is presented as one row with features set.\n",
    "    conv1 = Conv1D(filters = 64, kernel_size= 2, strides = 1, padding= \"same\", activation = 'relu')(x)  # Convoluting using 64 filters\n",
    "    \n",
    "    # Pooling layer to select the one value from the filter application on each row in a sliding manner, insetad fo keeping whole grid.\n",
    "    # We can treat this also as another way of dropout whereby we drop soem information and keep most relevant information.\n",
    "    pool1 = MaxPool1D(pool_size=1, padding = \"same\")(conv1)            \n",
    "\n",
    "    conv2 = Conv1D(filters = 32, kernel_size= 2, strides = 1, padding= \"same\", activation = 'relu')(pool1) # Convoluting using 32 filters\n",
    "    # As we dont use pooling here, instead we try a regularization by means of dropout.\n",
    "    # using second layer of convolution is like using the the learned simple representation of data by filters in previous layer to build \n",
    "    # more sophisticated representation\n",
    "    # Ex: Maybe previous filters learn how price difference interacts with categorical variable like day, holiday, weekend (JUST Yes/NO). \n",
    "    # But in next layer it could be learning the order of interaction, is it linear or some non-linear adn order.\n",
    "    # From analogy of image, Conv1 learns line but in conv2 it uses them to learn about shapes like circle, square etc.\n",
    "    dropout2 = Dropout(0.2)(conv2)\n",
    "    drop_flat = Flatten()(dropout2)   # flattening the data to be sent to FC Dense network\n",
    "    dense1 = Dense(256, activation='relu')(drop_flat)   # we use 256 hidden units of the FC.\n",
    "\n",
    "    # Acitvation linear is used here because we explicitly want the model to predict the the target variable which can ahve range value (-inf,inf)\n",
    "    # This is a regression problem, therefore any fucntion which has range space of (-inf,inf) can be used in the last Dense layer\n",
    "    predictions = Dense(1,activation= 'linear')(dense1)   \n",
    "    model = Model(inputs = x, outputs = predictions)\n",
    "    print(model.summary())  # to see the model achitecture and params \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to see the architecture of Model\n",
    "CNN_basemodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LSTM model. defualt value of n_features is kept for time_window =5, which can be overwittten by passing feature value \n",
    "# for time_window = 15\n",
    "def LSTM_base(n_features = 41):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, LSTM, Input\n",
    "    from tensorflow.keras import backend as be\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "    from tensorflow.keras.optimizers import Adam, SGD\n",
    "    import absl.logging\n",
    "    from tensorflow.keras.models import Model\n",
    "    \n",
    "    x = Input(shape=(1,n_features))   \n",
    "    # As we are feeding one  LSTM into another we pass the return sequence which is the sequence of hidden states for each time step\n",
    "    #  rather than last hidden state. Just like CNN explanation above we can imagine similar effect happening with stacked LSTM\n",
    "     # https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
    "    # # https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "\n",
    "    lstm1 = LSTM(units = 100,dropout= 0.2, return_sequences= True)(x)\n",
    "    lstm2 = LSTM(10)(lstm1)\n",
    "\n",
    "    # from the LSTM network we feed the output to feed Forward netowrk and use 'linear' activation in last prdiction layer as target is between(-inf,inf)\n",
    "   \n",
    "    dense1 = Dense(256, activation='relu')(lstm2)\n",
    "    predictions = Dense(1, activation='linear')(dense1)\n",
    "    model = Model(inputs = x, outputs = predictions)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to see the architecture of Model\n",
    "LSTM_base(n_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clear the session\n",
    "be.clear_session()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# creating variable to save the output from run of each model for each time-window\n",
    "report = pd.DataFrame(columns = ['epochs','batch_size','optimizer','activation',\"Model\",'Val_ms_error','Val_ma_error','Time_window'])\n",
    "\n",
    "Time_windows = [5,15]    # Data given has two set of time_window\n",
    "\n",
    "# for loop to iterate through both timewindow and \n",
    "for i in Time_windows:\n",
    "    \n",
    "    batch_size = 100   # keeping batch size 100 for to see how model performs\n",
    "    epoch = 20   # keeeping epoch low to see if any convergence happening or not.\n",
    "    X_train, y_train, X_valid, y_valid = load_data(i) # load data set by iterating through Time_window\n",
    "    n_features = X_train.shape[1]   \n",
    "    X_train = X_train.values.reshape(X_train.shape[0],1,n_features)  # reshaping the data as LSTM expects data in 3D\n",
    "    X_valid = X_valid.values.reshape(X_valid.shape[0],1,n_features)   \n",
    "\n",
    "    model = CNN_basemodel(n_features = n_features)  #calling base model \n",
    "    model.compile(optimizer='adam', loss='mse')  # using 'Adam' optimzer , as we did gridsearch \n",
    "    # ( on reduced data set, taking X_train[0:30000] as train and X_train[30000:36000] as test) in which Adam seemed to \n",
    "    # perform better, though marginally so it doesnt matter as of now.\n",
    "    history = model.fit(X_train,y_train,validation_data = (X_valid, y_valid), \n",
    "                        batch_size = batch_size,epochs=epoch, verbose=1,shuffle = False)\n",
    "    \n",
    "    display_history(history,\"CNN_BASE\")   # display train and val loss fucntion across the epoch\n",
    "    mse_error, mae_error = evaluate_model(model,X_valid,y_valid)\n",
    "\n",
    "    # appending results of this iteration to report data frame created to keep track of same. Though we are using the last model state \n",
    "    # in the report, by looking at graph we can check if we need to save the best model state. Though in this problem we are not savinng best model\n",
    "    # as there is very marginal improvment which can be seen from log and grarph.\n",
    "\n",
    "    report.loc[len(report)] = [epoch,batch_size,\"Adam\",\"RELU\",\"CNN_Basic\",mse_error,mae_error,i]\n",
    "\n",
    "    # MAE is more robust to data with outliers and MSE penalizes the outliers a lot more due to value being raised to power(2). \n",
    "    print(\"Mean squared error on valid is: {}\".format(mse_error))  # getting both MAE and MSE as required by problem statement.\n",
    "    print(\"MAE on valid is: {}\".format(mae_error))\n",
    "\n",
    "    # Calling LSTM model.\n",
    "    model = LSTM_base(n_features = n_features)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    history = model.fit(X_train,y_train,validation_data = (X_valid, y_valid), \n",
    "                        batch_size = batch_size,epochs=epoch, verbose=1,shuffle = False)\n",
    "    display_history(history,\"LSTM_BASE\")\n",
    "    mse_error, mae_error = evaluate_model(model,X_valid,y_valid)\n",
    "    report.loc[len(report)] = [epoch,batch_size,\"Adam\",\"RELU\",\"LSTM_Basic\",mse_error,mae_error,i]\n",
    "    print(\"Mean squared error on valid is: {}\".format(mse_error))  \n",
    "    print(\"MAE on valid is: {}\".format(mae_error))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv('LSTM_CNN_BASE.csv')  # to save the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having tried the CNN and LSTM we see that dataset is so complex and ahs inertia, thereby lets try to implement Ensemble method like Random forest. Therefore we do the grid search from start for Random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor  #try to figure out number of estimators required\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "RForregCV = RandomForestRegressor(random_state=10)\n",
    "param_grid = {'n_estimators': [100, 200, 300,400], # the number of trees\n",
    "              'max_depth': [2,4,8],\n",
    "              'max_features': ['auto', 'log2'] #features in model\n",
    "              }\n",
    "X_train, y_train, X_valid, y_valid = load_data(5)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "\n",
    "# we have used the Timeseriesplit as data is temporal in nature and in Cross validation time its important\n",
    "# to tell split data and use CV in a way which keeps thsi temporal nature intact.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html\n",
    "my_cv = TimeSeriesSplit(n_splits=2)   \n",
    "\n",
    "# mean squsred errror is not provded as scoring method instead it has 'neg_mean_squared_error' which is replacment for MSE\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "CV_rfmodel = GridSearchCV(estimator=RForregCV, param_grid=param_grid, cv = my_cv, scoring = 'neg_mean_squared_error', verbose = 3)\n",
    "CV_rfmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "RForregCV = RandomForestRegressor(random_state=10)\n",
    "#since lowest estimators was better than we use lower estimators and max feature combo, also we see the best result for n_estimators = 100\n",
    "# therefore we increse the n_estimators value to see if there is further scope fo improvement\n",
    "param_grid = {'n_estimators': [10, 20,40, 100], \n",
    "              'max_depth': [2,4,8,16],\n",
    "              'max_features': ['auto', 'log2']\n",
    "              }\n",
    "X_train, y_train, X_valid, y_valid = load_data(5)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_rfmodel = GridSearchCV(estimator=RForregCV, param_grid=param_grid, cv = my_cv, scoring = 'neg_mean_squared_error', verbose = 3)\n",
    "CV_rfmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CV_rfmodel.best_score_, CV_rfmodel.best_params_)  # getting best params\n",
    "evaluate_model(CV_rfmodel, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model for window 5 turned out to be max depth 2 max_features = log2 and n_estimators 10, similarly we do the same with time window 15.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "RForregCV = RandomForestRegressor(random_state=10)\n",
    "param_grid = {'n_estimators': [100, 200, 300,400], #we try same grid search for 15 window\n",
    "              'max_depth': [2,4,8,16],\n",
    "              'max_features': ['auto', 'log2']\n",
    "              }\n",
    "X_train, y_train, X_valid, y_valid = load_data(15)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_rfmodel = GridSearchCV(estimator=RForregCV, param_grid=param_grid, cv = my_cv, scoring = 'neg_mean_squared_error', verbose = 3)\n",
    "CV_rfmodel.fit(X_train, y_train)\n",
    "print(CV_rfmodel.best_score_, CV_rfmodel.best_params_)\n",
    "evaluate_model(CV_rfmodel, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "RForregCV = RandomForestRegressor(random_state=10)\n",
    "\n",
    "# from previous analysis of window 5 lower values of n estimators perform better so we try those\n",
    "# max_depth also we searched on lower side to see if we can get better results.\n",
    "\n",
    "param_grid = {'n_estimators': [2,3,4,5,6,7],    \n",
    "              'max_depth': [1,2,3,4],        \n",
    "              'max_features': ['auto', 'log2']\n",
    "              }\n",
    "X_train, y_train, X_valid, y_valid = load_data(15)  # time window 15\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_rfmodel = GridSearchCV(estimator=RForregCV, param_grid=param_grid, cv = my_cv, scoring = 'neg_mean_squared_error', verbose = 3)\n",
    "CV_rfmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CV_rfmodel.best_score_, CV_rfmodel.best_params_)\n",
    "evaluate_model(CV_rfmodel, X_valid, y_valid)\n",
    "#BEST MSE for Random forest are seen here. best params best params{'max_depth': 1, 'max_features': 'log2', 'n_estimators': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest has produced good results with low amounts value for n_estimators and max_depth and max_features = log2. We go to XGboost to see if we can improve or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first grid search for XG boost with varied max depths of [2,5,12], varied n estimators of [10,50,100,150] and learning rates of [0.1, 0.01,0.05]\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "xgbreg = xgb.XGBRegressor()\n",
    "param_grid = {'max_depth': [2,5,12], \n",
    "              'n_estimators': [10, 50, 100, 150],\n",
    "              'learning_rate': [0.1, 0.01, 0.05]}\n",
    "X_train, y_train, X_valid, y_valid = load_data(5)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_xgbmodel = GridSearchCV(estimator=xgbreg, param_grid=param_grid, cv = my_cv, verbose = 2) #n_jobs = -1\n",
    "CV_xgbmodel.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CV_xgbmodel.best_score_, CV_xgbmodel.best_params_)\n",
    "\n",
    "evaluate_model(CV_xgbmodel, X_valid, y_valid)\n",
    "#best MAE window 5 {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xgbreg = xgb.XGBRegressor()\n",
    "param_grid = {'max_depth': [2,5,12], \n",
    "              'n_estimators': [10, 50, 100, 150],\n",
    "              'learning_rate': [0.1, 0.01, 0.05]}\n",
    "X_train, y_train, X_valid, y_valid = load_data(15)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_xgbmodel = GridSearchCV(estimator=xgbreg, param_grid=param_grid, cv = my_cv, verbose = 2) #n_jobs = -1\n",
    "CV_xgbmodel.fit(X_train, y_train)\n",
    "#the first grid search for XG boost window size 15 with varied max depths of [2,5,12], varied n estimators of [10,50,100,150] and learning rates of [0.1, 0.01,0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CV_xgbmodel.best_score_, CV_xgbmodel.best_params_)\n",
    "evaluate_model(CV_xgbmodel, X_valid, y_valid)\n",
    "#The results for the values of learning rate, max_depth and n_estiators are same for both the window sizes\n",
    "#The values are close to the dummy regressor values\n",
    "#Best MSE window 15 {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xgbreg = xgb.XGBRegressor()\n",
    "param_grid = {'max_depth': [2,3,4,5,6], \n",
    "              'n_estimators': [10, 20, 30, 40],\n",
    "              'learning_rate': [0.1, 0.2, 0.3, 0.4]}\n",
    "X_train, y_train, X_valid, y_valid = load_data(5)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_xgbmodel = GridSearchCV(estimator=xgbreg, param_grid=param_grid, cv = my_cv, verbose = 2) #n_jobs = -1\n",
    "CV_xgbmodel.fit(X_train, y_train)\n",
    "#2nd Grid Search with parameters since we found the highest learning rate of 0.1 from previous grid search to work best we \n",
    "# tried values of [0.1, 0.2, 0.3, 0.4] and the n_estimators and max_depth were on the lower side so we try low n_estimator values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CV_xgbmodel.best_score_, CV_xgbmodel.best_params_) #lower estimators and higher learning rates tried still the best model is the same as previous model\n",
    "\n",
    "evaluate_model(CV_xgbmodel, X_valid, y_valid)\n",
    "#The results are similar to grid search 1 so not much improvement best params{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same values as above for window 15\n",
    "\n",
    "xgbreg = xgb.XGBRegressor()\n",
    "param_grid = {'max_depth': [2,3,4,5,6], \n",
    "              'n_estimators': [10, 20, 30, 40],\n",
    "              'learning_rate': [0.1, 0.2, 0.3, 0.4]}\n",
    "X_train, y_train, X_valid, y_valid = load_data(15)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_xgbmodel = GridSearchCV(estimator=xgbreg, param_grid=param_grid, cv = my_cv, verbose = 2) #n_jobs = -1\n",
    "CV_xgbmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CV_xgbmodel.best_score_, CV_xgbmodel.best_params_) #same results as the 1st grid search\n",
    "\n",
    "evaluate_model(CV_xgbmodel, X_valid, y_valid)\n",
    "#Best MAE window 15 best params{'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the time split in Time Series split\n",
    "\n",
    "xgbreg = xgb.XGBRegressor()\n",
    "param_grid = {'max_depth': [2,3,4], \n",
    "              'n_estimators': [10, 20],\n",
    "              'learning_rate': [0.1, 0.2, 0.3],\n",
    "              }\n",
    "X_train, y_train, X_valid, y_valid = load_data(5)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=3)\n",
    "CV_xgbmodel = GridSearchCV(estimator=xgbreg, param_grid=param_grid, cv = my_cv, verbose = 2, n_jobs = -1) #n_jobs = -1\n",
    "CV_xgbmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CV_xgbmodel.best_score_, CV_xgbmodel.best_params_) #with a varied time split still the model is performing a bit worse and doesn't have the best outputs even the mean squared error and mean absolute error increases in the model\n",
    "\n",
    "evaluate_model(CV_xgbmodel, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Even after trying multiple combinations with different learning rates [0.01, 0.01, 0,1, 0.2, 0.3, 0,4] and max depth [2, 3, 4, 5, 6, 12] and [10,20,30,40,50,100, 150] in addition to this different time spits were also tried of 2 and 3.\n",
    "#the best mae for window size 5 is 2.5 and window size 15 is 3.5 hence, there has not been much improvement with an xg boost model till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Grid Search for xgboost with low number of estimators and low max depths on window 5 as low values are performing great\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "xgbreg = xgb.XGBRegressor()\n",
    "param_grid = {'max_depth': [2,3,4,5,6], \n",
    "              'n_estimators': [2,4,5,6,7,8,9],\n",
    "              'learning_rate': [0.1, 0.2, 0.3, 0.4]}\n",
    "X_train, y_train, X_valid, y_valid = load_data(5)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_xgbmodel = GridSearchCV(estimator=xgbreg, param_grid=param_grid, cv = my_cv, verbose = 2) #n_jobs = -1\n",
    "CV_xgbmodel.fit(X_train, y_train)\n",
    "print(CV_xgbmodel.best_score_, CV_xgbmodel.best_params_) \n",
    "\n",
    "evaluate_model(CV_xgbmodel, X_valid, y_valid)\n",
    "#Best MSE window 5 best params{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Grid Search for xgboost with low number of estimators and low max depths on window 15\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "xgbreg = xgb.XGBRegressor()\n",
    "param_grid = {'max_depth': [2,3,4,5,6], \n",
    "              'n_estimators': [2,4,5,6,7,8,9],\n",
    "              'learning_rate': [0.1, 0.2, 0.3, 0.4]}\n",
    "X_train, y_train, X_valid, y_valid = load_data(15)\n",
    "y_train = y_train.values.reshape(y_train.shape[0],)\n",
    "my_cv = TimeSeriesSplit(n_splits=2)\n",
    "CV_xgbmodel = GridSearchCV(estimator=xgbreg, param_grid=param_grid, cv = my_cv, verbose = 2) #n_jobs = -1\n",
    "CV_xgbmodel.fit(X_train, y_train)\n",
    "print(CV_xgbmodel.best_score_, CV_xgbmodel.best_params_) \n",
    "\n",
    "evaluate_model(CV_xgbmodel, X_valid, y_valid)\n",
    "#There's a certain trend in XG boost the best performing models have low number of max depth and low number of n_estimators\n",
    "#and time split is also at its minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and CNN Models New Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen from the above analysis there is no convergence happening, we try to use the limited feature set to see \n",
    "# if we can atleast see some convergence \n",
    "# behaviour happening. We get columns with [open,high,low,close,vol,timefromlastrade].difference candles i.e\n",
    "#  columns named [0,1....23] in time_window 5 and [0,1....83] in other dataset by ignoring first 17 columns.\n",
    "def data_split(X_train, X_valid, time_window):\n",
    "    X_train = X_train.iloc[:,17:]\n",
    "    X_valid = X_valid.iloc[:,17:]\n",
    "\n",
    "    # for time_window we have (time_window -1) candles. We rehape the data to where the candles are stacked on each other\n",
    "    # [O C H L V DLT ]  - time t\n",
    "    # [O C H L V DLT ]   - time t+1\n",
    "    # thereby this becomes a temporal sequence with time steps = Time_window -1, with 6 features\n",
    "    \n",
    "    X_train = X_train.values.reshape(X_train.shape[0],time_window-1,6)  \n",
    "    X_valid = X_valid.values.reshape(X_valid.shape[0],time_window-1,6)\n",
    "    return X_train, X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model with temporal sequence \n",
    "# We feed each feature to one LSTM thereby parallezing this and then combine back the LSTM of each sequence.More can be  We flatten this output\n",
    "# and then feed it to Feedforward FC network. We also try to use different activation fucntion for Dense layer to see if it can break the inertia \n",
    "# that we see in the data set so far. We have used 'swish' and 'elu'. (though in notebook it is default 'swish')\n",
    "\n",
    "def LSTM_limited_model(time_window,n_features):\n",
    "  n_steps = time_window-1  # for each time window we have (time_window -1) candles this will be the time window value for transformed data set\n",
    "  # to make it more temporal sequential in inutition.\n",
    "\n",
    "  set_layers = []  # output layers set from each feature time sequence\n",
    "  inputs = []    # input layer for each sequence\n",
    "  for i in range(0,n_features):\n",
    "    \n",
    "    input = Input(shape = (n_steps,1)) #transforming shape of data to give to LSTM which excpects atleast 3D the third dimension being batch.\n",
    "    inputs.append(input)\n",
    "    layers = LSTM(units=10)(input)\n",
    "    layers = Flatten()(layers)\n",
    "    set_layers.append(layers)\n",
    "\n",
    "  merge = concatenate([x for x in set_layers])  # merge the output from each LSTM sequence\n",
    "  dense = Dense(100, activation='swish')(merge)\n",
    "  output = Dense(1, activation = 'linear')(dense)\n",
    "  model = Model(inputs=[x for x in inputs], outputs=output)  # we have six input sequence\n",
    "  print(model.summary())\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 10)           480         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 10)           480         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 10)           480         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 10)           480         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 10)           480         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 10)           480         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 10)           0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 10)           0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 10)           0           lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60)           0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          6100        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            101         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,081\n",
      "Trainable params: 9,081\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1cf0d19ef70>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_limited_model(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_limited_model1(time_window, n_features):\n",
    "  n_steps = time_window-1\n",
    "  set_layers = []  # output layers set from each feature time sequence\n",
    "  inputs = []     # input layer for each sequence\n",
    "\n",
    "  for i in range(0,n_features): \n",
    "    input = Input(shape = (n_steps, 1))\n",
    "    inputs.append(input)\n",
    "\n",
    "    # just like LSTM we apply convolution filter to these each sequence\n",
    "    layers = Conv1D(filters=64, kernel_size=2, activation='swish')(input)\n",
    "    layers = MaxPool1D(pool_size = 2)(layers)\n",
    "    layers = Flatten()(layers)\n",
    "    set_layers.append(layers)\n",
    "\n",
    "\n",
    "  merge = concatenate([x for x in set_layers])   # merge the output from each sequence\n",
    "  dense = Dense(100, activation='swish')(merge)\n",
    "  output = Dense(1)(dense)\n",
    "  model = Model(inputs=[x for x in inputs], outputs=output) \n",
    "  print(model.summary())\n",
    "  return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 4, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 3, 64)        192         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 3, 64)        192         input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 3, 64)        192         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 3, 64)        192         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 3, 64)        192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 3, 64)        192         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 64)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 64)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 64)        0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 64)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 64)        0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 64)        0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 64)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 64)           0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 64)           0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 64)           0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 64)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 64)           0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           flatten_7[0][0]                  \n",
      "                                                                 flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          38500       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            101         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 39,753\n",
      "Trainable params: 39,753\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x1cf0d7b4df0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_limited_model1(5,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report = pd.DataFrame(columns = ['epochs','batch_size','optimizer','activation',\"Model\",'Val_ms_error','Val_ma_error','Time_window'])\n",
    "\n",
    "Time_windows = [5,15]\n",
    "for i in Time_windows:\n",
    "    n_features = 1\n",
    "    batch_size = 100\n",
    "\n",
    "    epoch = 20\n",
    "    #loading data\n",
    "    X_train, y_train, X_valid, y_valid = load_data(i)\n",
    "    X_train, X_valid = data_split(X_train,X_valid,i)  # extracting the candle feature set and then roll the data to make each row sequential\n",
    "\n",
    "    model = CNN_limited_model1(i,X_train.shape[2])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # We need to give 6 inputs as there are 6 inputs sequnces so we rehape the data wehereby for timewindow 5 \n",
    "    # we ahve each row transformed as (24 -> (4,6))\n",
    "    history = model.fit([X_train[:, :, j].reshape(X_train.shape[0], X_train.shape[1], n_features) for j in range(X_train.shape[2])],\n",
    "                        y_train,\n",
    "                        validation_data = ([X_valid[:, :, j].reshape(X_valid.shape[0], X_valid.shape[1], n_features) for j in range(X_valid.shape[2])],\n",
    "                        y_valid), batch_size = batch_size,epochs=epoch, verbose=1,shuffle = False)\n",
    "\n",
    "    display_history(history,\"CNN_Limited_{}\".format(i))\n",
    "    mse_error, mae_error = evaluate_model(model,X_valid,y_valid)\n",
    "    report.loc[len(report)] = [epoch,batch_size,\"Adam\",\"swish\",\"CNN_Limited\",mse_error,mae_error,i]\n",
    "\n",
    "    model = LSTM_limited_model(i,X_train.shape[2])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    history = model.fit([X_train[:, :, j].reshape(X_train.shape[0], X_train.shape[1], n_features) for j in range(X_train.shape[2])],\n",
    "                    y_train,\n",
    "                    validation_data = ([X_valid[:, :, j].reshape(X_valid.shape[0], X_valid.shape[1], n_features) for j in range(X_valid.shape[2])],\n",
    "                                               y_valid), batch_size = batch_size,epochs=epoch, verbose=1,shuffle = False)\n",
    "\n",
    "    display_history(history,\"LSTM_Limited_{}\".format(i))\n",
    "    mse_error, mae_error = evaluate_model(model,X_valid,y_valid)\n",
    "    report.loc[len(report)] = [epoch,batch_size,\"Adam\",\"swish\",\"LSTM_Limited\",mse_error,mae_error,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far with these models we are not able to break the inertia of data set to make any kind of convergence, except some sporadic behaviour we see coming from ELU and Swish for CNN model. For further analysis we increased the epoch while apssing the 'swish' to see any convergence.So far we have not seen better convergence so we finally we decided to merge the CNN and LSTM to try something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 5\n",
    "X_train, y_train, X_valid, y_valid = load_data(time_window)\n",
    "# we separete the time series data from the categorical feature data set, we will feed the categorical to CNN and \n",
    "# the temporal to LSTM which takes input in shape of (n_steps = timewindow -1, features =6) [as explained earlier]. \n",
    "# However instead of treating the each time series as independent we treat them dependent and feed to one single LSTM layer.\n",
    "X_train_embed = X_train.iloc[:,:17]\n",
    "X_train_time  = X_train.iloc[:,17:]\n",
    "X_valid_embed = X_valid.iloc[:,:17]\n",
    "X_valid_time =  X_valid.iloc[:,17:]\n",
    "\n",
    "X_train_embed = X_train_embed.values.reshape(X_train_embed.shape[0],X_train_embed.shape[1],1) # reshape the data for dimensinal reason of tensorflow.\n",
    "X_valid_embed = X_valid_embed.values.reshape(X_valid_embed.shape[0],X_valid_embed.shape[1],1)\n",
    "\n",
    "X_train_time = X_train_time.values.reshape(X_train.shape[0],time_window-1,6) # reshape the data for dimensinal reason of tensorflow\n",
    "X_valid_time = X_valid_time.values.reshape(X_valid.shape[0],time_window-1,6)\n",
    "n_features = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating report frame\n",
    "report = pd.DataFrame(columns = ['epochs','batch_size','optimizer','activation','StdDeviation','LSTM1', 'LSTM2', 'learning_rate','weight constraint',\"Model\",'Val_ms_error','Val_ma_error','Time_window'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above we runn CNN on categorical data which we can terat as embeddings and time series data to LSTM and then merge them back. We run the LSTM on this again. The idea is LSTM will try to predict the next sequence of the temporal series and from CNN we get feature extraction, we again feed to this to LSTM because this CNN can do only feature extraction it cannot remeber the temporal importance between sequence of data and ehcne we use LSTM again and then feed this to Feedforward to predict the target variable.\n",
    "\n",
    "1. Recurrent Dropout: Recurrent Dropout is a regularization method for recurrent neural networks. Dropout is applied to the updates to LSTM memory cells (or GRU states), i.e. it drops out the input/update gate in LSTM/GRU\n",
    "\n",
    "Ref: https://paperswithcode.com/method/recurrent-dropout\n",
    "\n",
    "2. Batch Normalization: Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required.\n",
    "Ref: https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/ \n",
    "\n",
    "Kernel Initializer: It is used to initialize the initial weights as its importan t for symmetry breaking, as data has such a inertia we try to use this break this inertia.\n",
    "Through trial and error, we found that normal initializer works best for us with a stddev = 0.5.\n",
    "Normal Initializer: Parameter: Standard deviation (values of 0.1-1)\n",
    "\n",
    "Warm Restart: \n",
    "Why we used it?\n",
    "Since most of the outputs our neural network models were stuck in val loss, MSE and MAE values we thought of giving it a warm restart so that it could move out of a local minima it might have been stuck inside and find a new local minima for itself.This effect can be understood with temperature parameter. Initially the system is at higher temperature state and hence more smooth the topology of loss fucntion. At start, the model is able to explore the space more as  it is smooth, however when the temprature reduces (learning rate reduces) the model gets stuck in a confined local space of the loss function and starts finding the local minima.By doing warm restart we again make the toplogy smooth and in a way allow model to go to other part of topology and explore the minima in that confined space. Repeatedly doing this activity we are insured that we end-up finding the better local minima.\n",
    "It is evidently seen that by using the warm restart by keeping other paprmaters fixed the learning can improve significantly. [paper](https://arxiv.org/pdf/1608.03983.pdf)\n",
    "\n",
    "\n",
    "Stateful: RNNs can be stateful, which means that they can maintain state across batches during training. That is, the hidden state computed for a (ith element of batch of training data will be used as the initial hidden state for the ith data of next batch of training data. This is set to False by defualt as RNN treats each batch more like a sentence and resets it state for next batch. We tried the same by putting the stateful = True, and making batch size = 1. In temporal continous data we need stateful = True or the batch size should be chosen in a mannner where we see the autocorrelation.\n",
    "https://machinelearningmastery.com/stateful-stateless-lstm-time-series-forecasting-python/\n",
    "\n",
    "Different activations we used and why we used different activations and their results?\n",
    "The best activation function we found by trial and error was ReLU. \n",
    "Different out of ordinary activation functions tried:\n",
    "These gave out some interesting results but dont give convergence.\n",
    "\n",
    "Weight constraint: A weight constraint is an update to the network that checks the size of the weights, and if the size exceeds a predefined limit, the weights are rescaled so that their size is below the limit or between a range.\n",
    "Ref: https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  \n",
    "lstm_cell1 = 100  # LSTM cell size for first \n",
    "epoch = 10\n",
    "p = 5   # weight constraint variable\n",
    "stddev = 0.5   # stddev variable to be used for symmetryy breaking while kernel initialization usinf RandomNormal\n",
    "learning_rate = 0.01   # for optmizer as we are implementing the warm restart with cosine decay we can keep inital value high.\n",
    "filter1 = 16 \n",
    "lstmcell2 = 100\n",
    "rec_dropout = 0.5\n",
    "# def final_model():\n",
    "\n",
    "# this takes the temporal data\n",
    "x1 = Input(shape = (time_window-1,n_features) )   \n",
    "# with the training period the weights shift which could be their true representaiton, hence we use batch normalization to avoid this effect \n",
    "x1 = BatchNormalization()(x1)\n",
    "lstm1 = LSTM(units = lstm_cell1, bias_initializer = initializers.RandomNormal(stddev= stddev))(x1)  \n",
    "flat1 = Flatten()(lstm1)\n",
    "\n",
    "# this takes the categorical/ordinal and rest other from candles\n",
    "x2 = Input(shape = (X_train_embed.shape[1],1))\n",
    "conv1 = Conv1D(filters = filter1, kernel_size= 3, strides = 1, padding= \"same\", activation = 'relu', kernel_constraint=maxnorm(p))(x2)\n",
    "maxpool1 = MaxPool1D(pool_size = 2)(conv1)\n",
    "flat2 = Flatten()(maxpool1)\n",
    "merge = concatenate([flat1,flat2]) #concatenating the outputs from LSTM and CNN\n",
    "merge = Reshape((228,1))(merge)\n",
    "\n",
    "# next LSTM layer with features extracted by CNN and LSTM predicting the next eleement of temporal sequence data.\n",
    "lstm2 = LSTM(units = lstmcell2, recurrent_dropout = rec_dropout, kernel_constraint=maxnorm(p), recurrent_constraint= maxnorm(p),\n",
    "            kernel_initializer = initializers.RandomNormal(stddev= stddev))(merge)\n",
    "\n",
    "# lstm2 = LSTM(10)(lstm1)\n",
    "dense1 = Dense(256, activation='relu')(lstm2)\n",
    "predictions = Dense(1, activation='linear')(dense1)\n",
    "model = Model(inputs = ([x1,x2]), outputs = predictions)\n",
    "\n",
    "model.compile()\n",
    "print(model.summary())\n",
    "\n",
    "# implementing warm restart using Cosine decay.\n",
    "# decay steps in first cycle. So now the first warm restart happens at epoch 3 and then it becomes 1.5*3 and so on...\n",
    "# # Cosinedecay with restart, decay_steps(cycle = n) = t_mul*decay_steps (cycle = n-1)\n",
    "first_decay_steps = (int(X_train.shape[0]/batch_size))*3 \n",
    "lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecayRestarts(learning_rate,\n",
    "                                        first_decay_steps, t_mul=1.5)\n",
    "cp1 = ModelCheckpoint(\"CNN_LSTM\"+'/', save_best_only=True, save_format=\"h5\")        \n",
    "model.compile(optimizer=Adam(learning_rate= lr_decayed_fn), loss='mse')\n",
    "history = model.fit([X_train_time, X_train_embed],y_train,validation_data = ([X_valid_time,X_valid_embed],y_valid), \n",
    "                    batch_size = batch_size,epochs=epoch, verbose=1, callbacks=[cp1], shuffle = False)\n",
    "\n",
    "display_history(history,\"LSTM_CNN\")\n",
    "\n",
    "mse_error, mae_error = evaluate_model(model,[X_valid_time,X_valid_embed],y_valid)\n",
    "report.loc[len(report)] = [epoch,batch_size,\"Adam\",\"RELU\",stddev,lstm_cell1,lstmcell2,learning_rate,p,\"LSTM_CNN\", mse_error,mae_error,5]\n",
    "print(\"Mean squared error on valid is: {}\".format(mse_error))\n",
    "print(\"MAE on valid is: {}\".format(mae_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention:\n",
    "\n",
    "EXTRA Code block from grid search and other paramter exploration. this might break if tried to run with this code.as might need soem data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model(optimizer='adam', pool_type = 'max', activation = \"relu\", dropout_rate_1 = None, dropout_rate_2 = None, \n",
    "                    filter1 = 64, filter2 = 32, stride1 = (1,1), stride2 = (1,1), learning_rate = 0.001, \n",
    "                    stddev =1,weight_constraint =1, kernel1 = (5,5), kernel2 = (2,2), neurons = 256):\n",
    "\n",
    "    from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPool2D, Flatten, AveragePooling2D\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.optimizers import Adadelta, Adam, SGD\n",
    "    from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "    from tensorflow.keras.regularizers import l1\n",
    "    from tensorflow.keras.backend import clear_session\n",
    "    from tensorflow.keras.callbacks import TensorBoard\n",
    "    from tensorflow.keras.initializers import glorot_normal\n",
    "    from keras.constraints import maxnorm\n",
    "    from tensorflow.keras import initializers\n",
    "    import tensorflow as tf\n",
    "\n",
    "\n",
    "    x = Input(shape = (24,41,1))\n",
    "    conv1 = Conv2D(filters = filter1, kernel_size= kernel1, strides = stride1, padding= \"same\", activation = 'relu',\n",
    "                    kernel_constraint=maxnorm(weight_constraint),kernel_initializer=initializers.RandomNormal(stddev= stddev) )(x)\n",
    "\n",
    "    if pool_type == 'max':\n",
    "        pool1 = MaxPool2D(pool_size=(4, 4), padding = \"same\")(conv1)\n",
    "    if pool_type == 'average':\n",
    "        pool1 = AveragePooling2D(pool_size=(4, 4), padding = \"same\")(conv1)\n",
    "    dropout1 = Dropout(dropout_rate_1)(pool1)\n",
    "\n",
    "    conv2 = Conv2D(filters = filter2 , kernel_size= kernel2, strides = stride2, padding= \"same\", activation = 'relu',\n",
    "                    kernel_constraint=maxnorm(weight_constraint),kernel_initializer=initializers.RandomNormal(stddev= stddev) )(x)\n",
    "\n",
    "    dropout2 = Dropout(dropout_rate_2)(conv2)\n",
    "    drop_flat = Flatten()(dropout2)\n",
    "\n",
    "    dense1 = Dense(neurons, activation='relu')(drop_flat)\n",
    "    predictions = Dense(1,activation= 'linear')(dense1)\n",
    "    model = Model(inputs = x, outputs = predictions)\n",
    "    model.compile(optimizer=\"Adam\", loss= \"mse\", metrics=[tf.keras.metrics.MeanSquaredError(), tf.keras.losses.MeanAbsoluteError()])\n",
    "\n",
    "    # model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code Block we used for grid search###\n",
    "optimizer = ['Adam','Nadam', 'RMSprop', 'Adadelta']\n",
    "pool_type = ['max']  , ['average']\n",
    "filter1=[64]      #   [128,32]\n",
    "filter2=[32]      #   [32,16]\n",
    "stride1=[(2,2)]  #[(1,1),(2,2),(3,3)],\n",
    "stride2 =[(2,2)]   #[(1,1),(2,2),(3,3)],\n",
    "neurons=[128]\n",
    "learning_rate =[0.1]#,0.001,0.005],  \n",
    "weight_constraint = [1,3]  # [1,3]\n",
    "stddev =[0.5] #[0.1,0.5,1], #[32, 256],\n",
    "dropout_rate_1 = [ 0.20]#,0.30],\n",
    "dropout_rate_2= [ 0.30]\n",
    "kernel1 = [(5,5)]\n",
    "kernel2 = [(2,2)]\n",
    "activation = [\"relu\"]\n",
    "batch_size = [100] # [32 100]\n",
    "epochs = [50]  # [50, 100]\n",
    "\n",
    "\n",
    "for i in batch_size:   # ok\n",
    "    for j in epochs:    #\n",
    "        for k in optimizer:#\n",
    "            for l in learning_rate: #\n",
    "                for m in dropout_rate_1:  #\n",
    "                    for n in neurons:  #\n",
    "                        for o in stddev:  #\n",
    "                            for p in weight_constraint:    #\n",
    "                              for q in dropout_rate_2:   #\n",
    "                                for r in pool_type:\n",
    "                                  for s in stride1:\n",
    "                                    for t in stride2:\n",
    "                                      for u in filter1:\n",
    "                                        for v in filter2:\n",
    "                                          for w in kernel1:\n",
    "                                            for z in kernel2:\n",
    "                                                be.clear_session()\n",
    "\n",
    "                                                def schedule(epoch, lr = l): #a decaying learning rate with this function\n",
    "                                                  if epoch < 5:\n",
    "                                                    return lr\n",
    "                                                  else:\n",
    "                                                    return lr * tf.math.exp(-0.2)\n",
    "                                                lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "                                                model = KerasRegressor(build_fn=  lambda: CNN_model(optimizer=k, pool_type = r, activation = \"relu\", \n",
    "                                                          dropout_rate_1 = m, dropout_rate_2 = q, filter1 = u, \n",
    "                                                          filter2 = v, stride1 = s, stride2 = t, learning_rate = l, \n",
    "                                                          stddev = o,weight_constraint =p, kernel1 = w, kernel2 = z, \n",
    "                                                          neurons = n), epochs=j, batch_size=i, verbose=2, shuffle = False)\n",
    "                                                history = model.fit(X_train, y_train, batch_size = i,epochs=j, callbacks=[lr_scheduler], shuffle = False)\n",
    "                                                mse_error, mae_error = evaluate_model(model,X_valid_rolled,y_valid_rolled)\n",
    "                                                report.loc[len(report)] = [j,i,k,r,\"RELU\",m,q,u,v,s,t,l,o,p,w,z,n,mse_error,mae_error]\n",
    "                                                print(\"Mean squared error on valid is: {}\".format(mse_error))\n",
    "                                                print(\"MAE on valid is: {}\".format(mae_error))\n",
    "\n",
    "                                  # def fit_lstm(train, batch_size, nb_epoch, neurons):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Grid Model\n",
    "\n",
    "X_train_rolled, y_train_rolled ,segmenter = data(X_train[:30000],y_train[:30000],time_window=time_window,step=1,forecast_distance=1)\n",
    "X_valid_rolled, y_valid_rolled, segmenter = data(X_train[30000:36000],y_train[30000:36000],step =1, time_window= time_window, \n",
    "                                                            forecast_distance=1)\n",
    "\n",
    "#                 }     \n",
    "batch_size = [200]  #  \n",
    "epochs = [5]        #\n",
    "optimizer = ['Adam']   # Dont change this\n",
    "learning_rate = [0.01]             # dont change the this\n",
    "dropout = [0.3]   \n",
    "recurrent_dropout = [0.3]                         # \n",
    "units = [100]                                     # \n",
    "stddev = [1]               # dont change this             \n",
    "weight_constraint = [3]\n",
    "stateful = [False]          # dont change this\n",
    "\n",
    "#                         # 1 weight\n",
    "# report = pd.DataFrame(columns=['Batch','epoch','optimizer','learning_rate','dropout','units',\n",
    "#                                 'stddev','weight_constraint', 'Val_ms_error', 'Val_ma_error', 'Time_window','recuurent_dropout','Statefullness',])\n",
    "\n",
    "\n",
    "# i,j,k,l,m,n,\"Default\",p,mse_error, mae_error,time_window,q,r\n",
    "from numpy.random import seed\n",
    "seed(23)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "\n",
    "for i in batch_size:   # ok\n",
    "    for j in epochs:    #\n",
    "        for k in optimizer:#\n",
    "            for l in learning_rate: #\n",
    "                for m in dropout:  #\n",
    "                    for n in units:  #\n",
    "                        for o in stddev:  #\n",
    "                            for p in weight_constraint:    #\n",
    "                              for q in recurrent_dropout:   #\n",
    "                                for r in stateful:  #\n",
    "\n",
    "                                  be.clear_session()\n",
    "                                  def schedule(epoch, lr = l): #a decaying learning rate with this function\n",
    "                                    if epoch < 5:\n",
    "                                      return lr\n",
    "                                    else:\n",
    "                                      return lr * tf.math.exp(-0.2)\n",
    "                                  lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "                                  # def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\n",
    "                                  #   model = Sequential()\n",
    "                                  #   model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "                                  #   model.add(Dense(1))\n",
    "                                  #   model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "                                  #   for i in range(nb_epoch):\n",
    "                                  #     model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "                                  #     model.reset_states()\n",
    "                                  #   return model\n",
    "\n",
    "                                  model = Sequential()\n",
    "\n",
    "\n",
    "                                  model.add(LSTM(units = n, dropout = m, recurrent_dropout = q,input_shape = (time_window,column_count),\n",
    "                                                  kernel_constraint=maxnorm(p), recurrent_constraint= maxnorm(p), stateful= r,  ))       # batch_input_shape = (i,time_window,column_count)))\n",
    "                                  model.add(Dense(1,'linear'))\n",
    "                                  # cp1 = ModelCheckpoint('name'+'/', save_best_only=True, save_format=\"h5\")\n",
    "                                  model.compile(loss='mean_squared_error', optimizer= k )\n",
    "\n",
    "                                  # history = model.fit(X_train_rolled, y_train_rolled,validation_data=(X_valid_rolled, y_valid_rolled),epochs=epochs, \n",
    "                                  #                     batch_size=batch_size, shuffle = False)\n",
    "                                  history = model.fit(X_train_rolled, y_train_rolled, batch_size = i,epochs=j, callbacks=[lr_scheduler], shuffle = False)\n",
    "\n",
    "                                  # predict_model = Sequential([Input(batch_input_shape=(1,...),\n",
    "                                  # <continue specifying exact same model>])\n",
    "\n",
    "                                  # model.save_weights('lstm_model.h5')\n",
    "\n",
    "#                                   predict_model = Sequential()\n",
    "\n",
    "\n",
    "#                                   predict_model.add(LSTM(units = n, dropout = m, recurrent_dropout = q,\n",
    "#                                                   kernel_constraint=maxnorm(p), recurrent_constraint= maxnorm(p),stateful= r,batch_input_shape = (i,time_window,column_count)))\n",
    "#                                   predict_model.add(Dense(1,'linear'))\n",
    "#                                   # cp1 = ModelCheckpoint('name'+'/', save_best_only=True, save_format=\"h5\")\n",
    "#                                   model.compile(loss='mean_squared_error', optimizer= k )\n",
    "\n",
    "#                                   predict_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())\n",
    "#                                   # display_history(history,\"RMS(0.0005)_batch_30_cellsize(40)\")\n",
    "                                  mse_error, mae_error = evaluate_model(model,X_valid_rolled,y_valid_rolled)\n",
    "                                  report.loc[len(report)] = [i,j,k,l,m,n,\"Default\",p,mse_error, mae_error,time_window,q,r]\n",
    "                                  print(\"Mean squared error on valid is: {}\".format(mse_error))\n",
    "                                  print(\"MAE on valid is: {}\".format(mae_error))\n",
    "\n",
    "                                  ###### Dont ofregt to save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv(\"Grid_LSTM.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3472910ebd2aa62b6c54b483bd66ffdd88459b69fe7bc051f28b07b9e0034dc5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
